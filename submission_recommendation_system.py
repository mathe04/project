# -*- coding: utf-8 -*-
"""Submission_Recommendation_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lLtfGmTsmpYINnv1odZHX2l-8ofDn4iE

# **Business Understanding**

### I. Problem Statements
1. Bagaimana sistem dapat merekomendasikan buku-buku yang paling populer dan disukai secara umum oleh pengguna?
2. Dapatkah sistem menyaring dan merekomendasikan buku berkualitas tinggi berdasarkan penilaian dari reviewer ahli (pengguna aktif)?

### II. Goals
1. Menghasilkan daftar rekomendasi buku berdasarkan popularitas.
2. Menghasilkan daftar rekomendasi buku yang terkurasi, berdasarkan rating dari reviewer ahli yang telah memberikan ulasan dalam jumlah signifikan.

### III. Solution Statements
- Mengembangkan sistem rekomendasi berbasis popularitas (Content Based).
- Mengembangkan sistem filtering berbasis reviewer ahli dengan cara menyaring ulasan dari reviewer ahli (Filtering Based).
- Menyusun dua daftar rekomendasi:
1. Buku terpopuler secara umum.
2. Buku dengan rating tertinggi dari reviewer ahli.
- Sistem ini tidak bergantung pada histori personal pengguna sehingga sangat cocok untuk mengatasi masalah cold-start (pengguna baru).

# **Load Dataset**
"""

!pip install numpy==1.26.4 --force-reinstall --no-cache-dir

import os
os.kill(os.getpid(), 9)

import pandas as pd
import numpy as np
from IPython.display import Image, display
from sklearn.metrics.pairwise import cosine_similarity

print(np.__version__)

!pip install scikit-surprise

from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
from surprise import accuracy

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Unduh dan ekstrak dataset langsung
!kaggle datasets download -d arashnic/book-recommendation-dataset
!unzip -q book-recommendation-dataset.zip -d dataset

# Load data from CSV files
books = pd.read_csv('dataset/Books.csv')
users = pd.read_csv('dataset/Users.csv')
ratings = pd.read_csv('dataset/Ratings.csv')

"""# **Data Understanding**

## EDA
## Cek Informasi Tiap Variabel

Dataset yang digunakan terdiri dari 3 file berikut:
1. **Users:** Berisi fitur ID pengguna (`User-ID`), Lokasi Pengguna (`Location`), dan Usia (`Age`).
2.  **Books:** Berisi fitur `ISBN`, Judul Buku (`Book-Title`), Penulis Buku (`Book-Author`), Tahun Terbit (`Year-Of-Publication`), Penerbit (`Publisher`), dan URL yang menautkan ke gambar sampul terdapat tiga bentuk berbeda (`Image-URL-S`, `Image-URL-M`, dan `Image-URL-L`).
3. **Ratings:** Berisi fitur ID pengguna (`User-ID`), `ISBN`, dan Hasil Penilaian (`Book-Rating`) yang dinyatakan dalam skala 1-10 (nilai yang lebih tinggi menunjukkan apresiasi yang lebih tinggi).

### Users Variable
"""

users.info()

users.head()

"""Dataset `users` berisi informasi tentang pengguna, terdiri dari **278.858 baris** dan **3 kolom** dengan fitur:
1. **User-ID** bertipe data Integer. Merupakan identifikasi unik untuk setiap pengguna.
2. **Location** bertipe data Object (string). Berisi informasi lokasi pengguna dalam format `kota, provinsi, negara`.
3. **Age** bertipe data Float. Usia pengguna dicatat dalam satuan tahun.

### Books Variabel
"""

books.info()

books.head()

"""Dataset `books` terdiri dari **271.360 entri** dengan **8 kolom** yang semuanya bertipe *object* (string). Kolom-kolom ini mencakup informasi seperti:
1. `ISBN`: Nomor unik identifikasi buku.
2. `Book-Title`: Judul buku.
3. `Book-Author`: Nama penulis buku (2 nilai kosong).
4. `Year-Of-Publication`: Tahun terbit buku.
5. `Publisher`: Nama penerbit (2 nilai kosong).
6. `Image-URL-S`, `Image-URL-M`, `Image-URL-L`: URL gambar buku dalam tiga ukuran berbeda (3 nilai kosong pada URL-L).

Secara keseluruhan, dataset ini cukup bersih dan kaya akan informasi metadata buku yang bermanfaat untuk sistem rekomendasi berbasis konten maupun kolaboratif.

### Ratings Variable
"""

ratings.info()

ratings.head()

"""Dataset `ratings` terdiri dari **1.149.780 entri** dengan **3 kolom**:
1. `User-ID`: Identifikasi pengguna (int64).
2. `ISBN`: Identifikasi buku (object).
3. `Book-Rating`: Nilai rating yang diberikan (int64).
"""

ratings['Book-Rating'].describe()

"""Semua kolom tidak memiliki nilai kosong. Statistik dari kolom `Book-Rating` menunjukkan:
- **Mean**: 2.87
- **Standard Deviation**: 3.85
- **Minimum**: 0
- **Maximum**: 10
- **50% nilai rating (median)**: 0
    
Mayoritas nilai rating bernilai 0, mengindikasikan bahwa sebagian besar entri kemungkinan merupakan *implicit feedback* (misalnya buku dilihat tetapi tidak diberi rating eksplisit). Hal ini menunjukkan bahwa data rating bersifat sparse dan perlu penanganan lanjutan, seperti pemisahan antara rating eksplisit dan implisit, untuk meningkatkan kualitas hasil rekomendasi.

## Cek Missing Value

Dilakukan pengecekan missing value pada data `books`, karena data `books` merupakan data utama yang digunakan.
"""

books.isna().sum()

"""Dari hasil pengecekan mising value di atas, terdapat sebanyak 7 data, dengan perincian 2 data kosong pada fitur `Book-Author`, 2 data kosong pada fitur dan perlu dilakukan suatu penanganan."""

books[books.isnull().any(axis=1)]

"""## Cek Duplikasi Data

Dilakukan pengecekan apakah terdapat duplikasi data pada data `books`sebagai data utama.
"""

#  Mengecek duplikasi pada dataframe books
books.duplicated().sum()

"""Dari hasil pengecekan duplikasi data pada data `books`, tidak ditemukan adanya data yang duplikasi sehingga tidak diperlukan suatu penanganan.

## Hasil EDA

Dari hasil EDA yang dilakukan, dapat disimpulkan bahwa:
1. **Dataset yang digunakan**:
  - `ratings`: memberikan informasi tentang penilaian pengguna terhadap buku.
  - `books`: memberikan metadata buku, termasuk judul buku (`Book-Title`) yang digunakan sebagai kunci analisis.
2. **Dataset yang tidak digunakan**:
  - `users`: tidak dilibatkan dalam proses ini karena fokus analisis adalah pada buku, bukan pengguna.
"""



"""# **Data Preparation**

## Penanganan Missing Value

Pada EDA sebelumnya, diketahui bahwa data `books` terdapat missing value dan diperlukan penanganan agar mempermudah proses analisis.

 Digunakan penanganan **dropping missing values** dikarenakan jumlah data yang hilang pada data `books` sedikit dan apabila dilakukan penghapusan data maka hasil analisis tidak akan terpengaruh secara signifikan.
"""

books.dropna(inplace = True)

books.info()

"""Setelah dilakukan dropping missing values data `books` yang awalnya memiliki jumlah data sebanyak 271.360 menjadi 271.353 data.

## Menggabungkan Data Buku dengan Data Penilaian berdasarkan 'ISBN'

Langkah ini menggabungkan dataframe `ratings` dan `books` berdasarkan kolom ISBN. Hasilnya adalah dataframe `ratings_with_name` yang berisi data rating yang telah dilengkapi informasi buku seperti judul, penulis, dan penerbit.
"""

# Menggabungkan dataframe ratings dan books berdasarkan nilai 'ISBN'
ratings_with_name = ratings.merge(books, on='ISBN')
ratings_with_name

ratings_with_name.info()

"""## Train Test Data untuk Content Filtering

Tahapan ini membagi data interaksi pengguna dan buku (`ratings_with_name`) menjadi dua bagian: `train_data` (80%) untuk melatih sistem dan `test_data` (20%) untuk menguji efektivitas rekomendasi.
"""

from sklearn.model_selection import train_test_split

# Pisahkan data interaksi menjadi train & test
train_data, test_data = train_test_split(ratings_with_name, test_size=0.2)

# Proses untuk train_data
num_rating_train = train_data.groupby('Book-Title').count()['Book-Rating'].reset_index()
num_rating_train.rename(columns={'Book-Rating':'Num-Ratings'}, inplace=True)

avg_rating_train = (
    train_data.groupby('Book-Title')['Book-Rating']
    .mean()
    .reset_index()
)
avg_rating_train.rename(columns={'Book-Rating': 'Avg-Rating'}, inplace=True)

popular_books_train = num_rating_train.merge(avg_rating_train, on="Book-Title")

most_popular_books_train = popular_books_train[popular_books_train['Num-Ratings'] >= 400].sort_values('Avg-Rating', ascending=False)

most_popular_books_train = most_popular_books_train.merge(books, on="Book-Title").drop_duplicates('Book-Title')[
    ['Book-Title', 'Book-Author', 'Image-URL-M', 'Num-Ratings', 'Avg-Rating']
]

# Proses untuk test_data
num_rating_test = test_data.groupby('Book-Title').count()['Book-Rating'].reset_index()
num_rating_test.rename(columns={'Book-Rating':'Num-Ratings'}, inplace=True)

avg_rating_test = (
    test_data.groupby('Book-Title')['Book-Rating']
    .mean()
    .reset_index()
)
avg_rating_test.rename(columns={'Book-Rating': 'Avg-Rating'}, inplace=True)

popular_books_test = num_rating_test.merge(avg_rating_test, on="Book-Title")

most_popular_books_test = popular_books_test[popular_books_test['Num-Ratings'] >= 80].sort_values('Avg-Rating', ascending=False)

most_popular_books_test = most_popular_books_test.merge(books, on="Book-Title").drop_duplicates('Book-Title')[
    ['Book-Title', 'Book-Author', 'Image-URL-M', 'Num-Ratings', 'Avg-Rating']
]

# Inisialisasi hasil train dan test
relevant = most_popular_books_train['Book-Title'].head(10).tolist()
recommended = most_popular_books_test['Book-Title'].head(10).tolist()

"""**Hasil Akhir:**

- `relevant` merepresentasikan daftar buku populer dari data training, yang dianggap relevan sebagai acuan sistem rekomendasi.
- `recommended` merepresentasikan daftar buku rekomendasi hasil dari model menggunakan data testing.

Tujuannya adalah untuk membandingkan hasil rekomendasi model (recommended) dengan buku-buku yang memang populer dan disukai pengguna (relevant) dari data pelatihan, sebagai bagian dari proses evaluasi sistem.

## Mengambil Data Buku Populer Berdasarkan Reviewer Ahli dan Rating

Tujuan dari proses ini adalah untuk meningkatkan kualitas data yang digunakan dalam sistem rekomendasi dengan cara memfokuskan analisis hanya pada pengguna yang aktif (experienced users) dan buku yang populer di kalangan mereka. Dengan menyaring pengguna yang telah memberikan lebih dari 200 penilaian, sistem hanya mempertimbangkan opini dari reviewer yang memiliki pengalaman membaca yang cukup luas, sehingga penilaiannya dianggap lebih kredibel.

Selanjutnya, hanya buku yang telah menerima setidaknya 100 rating dari para pengguna aktif ini yang dipertahankan. Langkah ini menghindari buku-buku yang jarang dinilai (yang bisa menyebabkan bias atau noise) dan memastikan bahwa rekomendasi didasarkan pada data yang kuat dan representatif, baik dari sisi pengguna maupun buku.
"""

# Mengambil data User-ID dari data users yang telah menilai lebih dari 200 buku
x = ratings_with_name.groupby("User-ID").count()['Book-Rating'] > 200
experienced_users = x[x].index

# Melihat profil buku yang dinilai oleh experienved_users
filtered_ratings = ratings_with_name[ratings_with_name['User-ID'].isin(experienced_users)]
filtered_ratings

# Buku yang memiliki setidaknya 100 penilaian
y = filtered_ratings.groupby('Book-Title').count()['Book-Rating']>=100
famous_books = y[y].index

final_ratings = filtered_ratings[filtered_ratings['Book-Title'].isin(famous_books)]

final_ratings

"""**Hasil Akhir:**

Dataframe `final_ratings` berisi buku-buku yang dianggap populer, yaitu buku yang telah menerima setidaknya 100 rating dari pengguna berpengalaman (pengguna yang telah memberi lebih dari 200 rating).

# **Modeling dan Hasil**

Pada tahap modeling ini akan digunakan dua pendekatan sistem rekomendasi yang saling melengkapi untuk menghasilkan hasil yang lebih robust, yaitu:

1. **Content-Based Filtering**
  
  Pendekatan ini digunakan untuk merekomendasikan buku berdasarkan karakteristik atau atribut tertentu dari buku itu sendiri, dalam hal ini jumlah rating terbanyak dan rata-rata rating tertinggi. Buku yang sering dinilai dan mendapatkan rating tinggi diasumsikan memiliki kualitas atau daya tarik yang tinggi di mata banyak pengguna, sehingga layak untuk direkomendasikan secara umum. Pendekatan ini sangat berguna terutama saat informasi interaksi pengguna masih terbatas atau ketika ingin memberikan rekomendasi populer yang bersifat umum.

2. **Collaborative-Based Filtering (menggunakan SVD)**
    
  Pendekatan ini memanfaatkan data historis interaksi pengguna–dalam bentuk rating–untuk mempelajari pola preferensi dan merekomendasikan buku yang serupa dengan buku yang disukai oleh pengguna lain yang memiliki pola perilaku serupa. Dengan menerapkan algoritma Singular Value Decomposition (SVD), sistem dapat mengisi kekosongan data (sparse matrix) dan memprediksi rating yang mungkin diberikan oleh pengguna terhadap buku yang belum mereka nilai. Pendekatan ini lebih bersifat personal dan akurat karena mempertimbangkan pengalaman pengguna lain sebagai dasar rekomendasi.

Dengan menggabungkan kedua pendekatan ini, sistem dapat memberikan rekomendasi buku yang tidak hanya populer secara umum tetapi juga relevan secara personal bagi masing-masing pengguna.

## Content Filtering Based Recommendation System

Pada proses ini, digunakan pendekatan sederhana berbasis **content filtering** yang menyeleksi buku-buku berdasarkan dua fitur utama: **jumlah rating terbanyak** dan **rata-rata rating tertinggi**. Pendekatan ini mengasumsikan bahwa buku yang sering dinilai dan mendapat rating tinggi cenderung disukai oleh banyak pengguna, sehingga layak untuk direkomendasikan.
"""

# Melihat 10 profil buku rekomendasi yang memiliki penilaian terbanyak dengan nilai tertinggi
for i in range(10):
    print(f"\nBuku #{i+1}")
    print("Judul        :", most_popular_books_test['Book-Title'].iloc[i])
    print("Penulis      :", most_popular_books_test['Book-Author'].iloc[i])
    print("Jumlah Rating:", most_popular_books_test['Num-Ratings'].iloc[i])
    print("Rata-rata    :", round(most_popular_books_test['Avg-Rating'].iloc[i], 2))
    display(Image(url=most_popular_books_test['Image-URL-M'].iloc[i]))

"""## Collaborative Filtering Based Recommendation System (SVD Modeling)

Collaborative Filtering merupakan salah satu pendekatan paling populer dalam sistem rekomendasi yang berfokus pada pola interaksi antar pengguna dan item (dalam hal ini, buku). Pendekatan ini tidak memerlukan informasi konten dari buku, melainkan hanya mengandalkan rating atau feedback yang diberikan pengguna. Prinsip utamanya adalah: jika dua pengguna memiliki preferensi yang mirip di masa lalu, maka mereka cenderung menyukai item yang sama di masa depan.

Dalam proyek ini, collaborative filtering diterapkan menggunakan algoritma Singular Value Decomposition (SVD) dari library Surprise. SVD mampu mengatasi permasalahan umum dalam data rating seperti sparsity (banyaknya nilai kosong) dengan memetakan pengguna dan item ke dalam dimensi laten. Hasilnya adalah model yang dapat memprediksi seberapa besar kemungkinan seorang pengguna akan menyukai buku tertentu, meskipun belum pernah memberi rating untuk buku tersebut.
"""

from surprise.model_selection import train_test_split

# Define the rating scale
reader = Reader(rating_scale=(0, 10))

# Load the data into Surprise's dataset format
data = Dataset.load_from_df(final_ratings[['User-ID', 'Book-Title', 'Book-Rating']], reader)

# Split the dataset into training and testing sets
train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)

# Define the SVD algorithm
model = SVD()

# Train the algorithm on the training set
model.fit(train_set)

def recommend_books(user_id, n=10):
    # List all unique book titles
    all_books = final_ratings['Book-Title'].unique()

    # Remove books already rated by the user
    rated_books = final_ratings[final_ratings['User-ID'] == user_id]['Book-Title'].values
    books_to_predict = [book for book in all_books if book not in rated_books]

    # Predict ratings for remaining books
    predictions = []
    for book in books_to_predict:
        pred = model.predict(user_id, book)
        predictions.append((book, pred.est))

    # Sort predictions by estimated rating
    predictions.sort(key=lambda x: x[1], reverse=True)

    # Get top N recommendations
    top_n = predictions[:n]

    return top_n

from IPython.display import Image, display

# Make book recommendations
user_id = 271705
recommended_books = recommend_books(user_id)

print(f"Top 10 recommended books for user {user_id}:\n")
for i, (title, _) in enumerate(recommended_books, start=1):
    # Filter baris data berdasarkan judul
    book_info = final_ratings[final_ratings['Book-Title'] == title].iloc[0]

    # Ambil nama penulis dan URL gambar
    author_name = book_info['Book-Author'] if 'Book-Author' in book_info else "Tidak diketahui"
    image_url = book_info['Image-URL-M'] if 'Image-URL-M' in book_info else None

    # Tampilkan informasi
    print(f"Buku #{i}")
    print(f"Judul  : {title}")
    print(f"Penulis: {author_name}\n")

    # Tampilkan gambar jika URL tersedia
    if image_url:
        display(Image(url=image_url))

"""# **Evaluation**

Evaluasi dilakukan untuk mengukur kinerja dan efektivitas dari dua pendekatan sistem rekomendasi yang telah dibangun, yaitu Content-Based Filtering dan Collaborative Filtering berbasis SVD. Masing-masing pendekatan memiliki karakteristik dan metrik evaluasi yang berbeda, disesuaikan dengan tujuan dan data yang digunakan.

## 1. **Evaluasi Content-Based Filtering**

  Pada pendekatan content-based, sistem merekomendasikan buku berdasarkan jumlah rating terbanyak dan rata-rata rating tertinggi. Evaluasi dilakukan menggunakan dua metrik umum yaitu:
  
  1. Precision@K
  
  Precision@K mengukur seberapa relevan hasil rekomendasi yang diberikan, dengan rumus:

  $$
  \text{Precision@K} = \frac{|\text{Recommended}_k \cap \text{Relevant}|}{K}
  $$

  Semakin tinggi nilai precision, maka semakin besar proporsi buku yang direkomendasikan benar-benar relevan.

  2. Recall@K

  Recall@K mengukur seberapa banyak dari total item relevan yang berhasil direkomendasikan oleh sistem:

  $$
  \text{Recall@K} = \frac{|\text{Recommended}_k \cap \text{Relevant}|}{|\text{Relevant}|}
  $$

  Nilai recall yang tinggi menunjukkan sistem mampu menjangkau sebagian besar buku yang relevan.

## 2. **Evaluasi Collaborative-Based Filtering (SVD)**

  Berbeda dari pendekatan sebelumnya, collaborative filtering berbasis Singular Value Decomposition (SVD) tidak bergantung pada informasi konten buku, melainkan memanfaatkan interaksi historis antar pengguna dan item. Evaluasi dilakukan menggunakan metrik Root Mean Squared Error (RMSE), yang menghitung seberapa jauh prediksi sistem dari nilai rating aktual.

  Rumus matematis RMSE adalah:
  $$
  \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2}
  $$

  Di mana:  
  - $n$ adalah jumlah total prediksi,  
  - $\hat{y}_i$ adalah nilai prediksi ke-i,  
  - $y_i$ adalah nilai aktual ke-i.

## Evaluation untuk Content Filtering
"""

def precision_at_k(recommended_items, relevant_items, k):
    top_k = recommended_items[:k]
    relevant = set(top_k) & set(relevant_items)
    return len(relevant) / k

def recall_at_k(recommended_items, relevant_items, k):
    top_k = recommended_items[:k]
    relevant = set(top_k) & set(relevant_items)
    return len(relevant) / len(relevant_items)

print("Precision@3:", precision_at_k(recommended, relevant, 10))
print("Recall@3:", recall_at_k(recommended, relevant, 10))

"""**Hasil Akhir:**

- Precision@10 = 0.5 → 5 dari 10 rekomendasi termasuk buku relevan.
- Recall@10 = 0.5 → Dari 10 buku relevan, 5 berhasil direkomendasikan sistem.
- Metrik ini menunjukkan bahwa sistem mampu memberikan rekomendasi yang cukup tepat dan relevan terhadap preferensi pengguna berdasarkan karakteristik buku (misalnya judul dan genre).

## Evaluation untuk Collaborative Filtering dengan SVD
"""

# Make predictions on the test set
predictions = model.test(test_set)

# Evaluate the model
accuracy.rmse(predictions)

"""**Hasil Akhir**

- Nilai **RMSE yang lebih kecil** menunjukkan bahwa prediksi model lebih dekat dengan data aktual.
- Dalam hasil di atas, nilai **RMSE = 3.5880**, artinya secara rata-rata, prediksi model menyimpang sekitar 3.59 satuan dari nilai sebenarnya.
- Meskipun sistem dapat mempelajari pola dari data interaksi pengguna, nilai RMSE tersebut tergolong cukup besar, yang berarti akurasi prediksi rating masih kurang optimal. Hal ini bisa disebabkan oleh data yang sparse (jarang) atau kurangnya informasi pengguna/buku yang cukup beragam.

# Penutup

### Kesimpulan

Content-Based Filtering memberikan hasil yang lebih terkontrol dan dapat dijelaskan (explainable), karena berdasarkan informasi buku yang sudah diketahui.

Collaborative Filtering (SVD) berpotensi memberikan rekomendasi yang lebih personal, tetapi lebih rentan terhadap masalah data sparsity dan cold-start (pengguna/buku baru).

### Saran

Untuk hasil yang lebih optimal, dapat dipertimbangkan pendekatan hybrid yang menggabungkan kedua metode agar saling melengkapi kekurangan masing-masing.
"""